{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim.models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import KFold\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_reddit = r'C:\\Users\\henon\\code\\Stock_market\\STOCK_PREDICT\\data\\RedditNews.csv'\n",
    "path_news = r'C:\\Users\\henon\\code\\Stock_market\\STOCK_PREDICT\\data\\Combined_News_DJIA.csv'\n",
    "path_djia = r'C:\\Users\\henon\\code\\Stock_market\\STOCK_PREDICT\\data\\upload_DJIA_table.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit = pd.read_csv(path_reddit)\n",
    "news = pd.read_csv(path_news)\n",
    "djia = pd.read_csv(path_djia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge dataset\n",
    "news['Date'] = pd.to_datetime(news['Date'])\n",
    "djia['Date'] = pd.to_datetime(djia['Date'])\n",
    "\n",
    "df = news.merge(djia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get percentage change\n",
    "df['change'] = df['Open'].pct_change()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['change'] = df['change'].shift(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get categorical data\n",
    "def categorical(x):\n",
    "    if x > 0:\n",
    "        x = 1\n",
    "    else:\n",
    "        x = 0\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['target'] = df['change'].apply(categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the top 25 daily news into 1 column\n",
    "cols = df.columns[2:]\n",
    "df['combined'] = df[cols].apply(lambda row: '_'.join(row.values.astype(str)), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(text):\n",
    "    punctuation = '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'\n",
    "    for punctuation in punctuation:\n",
    "        review1 = text.replace(punctuation, ' ') # Remove Punctuation\n",
    "    lowercased = text.lower() # Lower Case\n",
    "    without_b=text.replace(\" b \",\"\")\n",
    "    without_b=text.replace(\"b'\",\"\")\n",
    "    without_b=text.replace('b\"',\"\")\n",
    "    tokenized = word_tokenize(without_b) # Tokenize\n",
    "    words_only = [word for word in tokenized if word.isalpha()] # Remove numbers\n",
    "    stop_words = set(stopwords.words('english')) # Make stopword list\n",
    "    without_stopwords = [word for word in words_only if not word in stop_words] # Remove Stop Words\n",
    "    lemma=WordNetLemmatizer() # Initiate Lemmatizer\n",
    "    lemmatized = [lemma.lemmatize(word) for word in without_stopwords] # Lemmatize\n",
    "    return \" \".join(lemmatized)\n",
    "df['cleaned'] = df['combined'].apply(clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       Georgia two Russian warplane country move brin...\n",
       "1       wont America Nato help u If wont help u help I...\n",
       "2       adorable sang opening ceremony That fake Georg...\n",
       "3       b refuse Israel weapon attack Iran president o...\n",
       "4       expert admit legalise drug South Osetia pictur...\n",
       "                              ...                        \n",
       "1984    Barclays RBS share suspended trading tanking s...\n",
       "1985    Scientists To Australia If You Want To Save Th...\n",
       "1986    Explosion At Airport In former president Terro...\n",
       "1987    Jamaica proposes marijuana dispenser tourist a...\n",
       "1988    A woman Mexico City finally received birth cer...\n",
       "Name: cleaned, Length: 1989, dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['cleaned']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
